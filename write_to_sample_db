pipeline {
    agent any

    environment {
        SPARK_HOME = '/home/hr295/spark'
        JDBC_JAR_PATH = '/home/hr295/spark/jars/postgresql-42.6.0.jar'
        PGSQL_USER = 'postgres'
        PGSQL_PASSWORD = 'access'
        JDBC_URL = 'jdbc:postgresql://192.168.1.77:5432/sample_database'
        SPARK_SCRIPT_PATH_1 = '/home/hr295/spark_files/script1.groovy' // First script
        SPARK_SCRIPT_PATH_2 = '/home/hr295/spark_files/script2.groovy' // Second script
    }

    stages {
        stage('Run First Spark Job') {
            steps {
                script {
                    sshagent(['spark-server-ssh-credential-id']) { // Replace with your SSH credentials ID
                        sh """
                        ssh -o StrictHostKeyChecking=no hr295@192.168.1.77 << 'EOF'
                        set -e
                        echo "Starting First Spark Job on Remote Server..."
                        
                        cat << 'GROOVY1' > ${SPARK_SCRIPT_PATH_1}
                        // Your first Groovy Spark script
                        import org.apache.spark.sql.SparkSession
                        import org.apache.spark.sql.functions.*

                        SparkSession spark = SparkSession.builder()
                            .appName("First Spark Job")
                            .config("spark.jars", "${JDBC_JAR_PATH}")
                            .getOrCreate()

                        println("Running the first job...")

                        spark.stop()
                        GROOVY

                        ${SPARK_HOME}/bin/spark-submit --class org.codehaus.groovy.tools.shell.Main ${SPARK_SCRIPT_PATH_1}
                        echo "First Spark job completed successfully."
                        EOF
                        """
                    }
                }
            }
        }

        stage('Run Second Spark Job') {
            steps {
                script {
                    sshagent(['spark-server-ssh-credential-id']) {
                        sh """
                        ssh -o StrictHostKeyChecking=no hr295@192.168.1.77 << 'EOF'
                        set -e
                        echo "Starting Second Spark Job on Remote Server..."
                        
                        cat << 'GROOVY2' > ${SPARK_SCRIPT_PATH_2}
                        // Your second Groovy Spark script
                        import org.apache.spark.sql.SparkSession
                        import org.apache.spark.sql.functions.*

                        SparkSession spark = SparkSession.builder()
                            .appName("Second Spark Job")
                            .config("spark.jars", "${JDBC_JAR_PATH}")
                            .getOrCreate()

                        println("Running the second job...")

                        spark.stop()
                        GROOVY

                        ${SPARK_HOME}/bin/spark-submit --class org.codehaus.groovy.tools.shell.Main ${SPARK_SCRIPT_PATH_2}
                        echo "Second Spark job completed successfully."
                        EOF
                        """
                    }
                }
            }
        }
    }

    post {
        success {
            echo "All Spark jobs executed successfully."
        }
        failure {
            echo "Failed to execute one or more Spark jobs."
        }
    }
}


